{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "Solution to 4(a)\n",
       "\n",
       "We are tasked with computing $\\mathbb{E}(y ; \\eta) = \\int_{-\\infty}^{\\infty}yp(y; \\eta) dy$. But a good place to start is with the hint provided in the question.\n",
       "\n",
       "\\begin{align*}\n",
       "\\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} p(y; \\eta) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} b(y) \\exp(\\eta y - a(\\eta)) (y - \\frac{\\partial a}{\\partial \\eta}) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} b(y) y \\exp(\\eta y - a(\\eta)) dy - \\frac{\\partial a}{\\partial \\eta}\\int_{-\\infty}^{\\infty} b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
       "    &= \\mathbb{E}(y ; \\eta) - \\frac{\\partial a}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy \\\\\n",
       "\\Rightarrow \\mathbb{E}(y ; \\eta) &= \\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy + \\frac{\\partial a}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy \\\\\n",
       "    &= \\frac{\\partial a}{\\partial \\eta}\n",
       "\\end{align*}\n",
       "\n",
       "The last equality follows from the fact that $\\int_{-\\infty}^{\\infty}p(y; \\eta) dy = 1$ since $p(y ; \\eta)$ is a probability density function.\n",
       "\n",
       "Two caveats: we were able to exchange the order of integration and differentiation in the first line by using \"Leibniz Integral Rule\" or \"Differentiation Under the Integral Sign\". There are technically some conditions that need to be checked for this rule to work but we'll ignore those details in this context. Secondly, we are dealing with the case of a continuous distribution here, which is implicit in the question wording (the discrete case would involve sums instead of integrals). \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "Solution to 4(a)\n",
    "\n",
    "We are tasked with computing $\\mathbb{E}(y ; \\eta) = \\int_{-\\infty}^{\\infty}yp(y; \\eta) dy$. But a good place to start is with the hint provided in the question.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} p(y; \\eta) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} b(y) \\exp(\\eta y - a(\\eta)) (y - \\frac{\\partial a}{\\partial \\eta}) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} b(y) y \\exp(\\eta y - a(\\eta)) dy - \\frac{\\partial a}{\\partial \\eta}\\int_{-\\infty}^{\\infty} b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
    "    &= \\mathbb{E}(y ; \\eta) - \\frac{\\partial a}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy \\\\\n",
    "\\Rightarrow \\mathbb{E}(y ; \\eta) &= \\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy + \\frac{\\partial a}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy \\\\\n",
    "    &= \\frac{\\partial a}{\\partial \\eta}\n",
    "\\end{align*}\n",
    "\n",
    "The last equality follows from the fact that $\\int_{-\\infty}^{\\infty}p(y; \\eta) dy = 1$ since $p(y ; \\eta)$ is a probability density function.\n",
    "\n",
    "Two caveats: we were able to exchange the order of integration and differentiation in the first line by using \"Leibniz Integral Rule\" or \"Differentiation Under the Integral Sign\". There are technically some conditions that need to be checked for this rule to work but we'll ignore those details in this context. Secondly, we are dealing with the case of a continuous distribution here, which is implicit in the question wording (the discrete case would involve sums instead of integrals). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "Solution to 4(b)\n",
       "\n",
       "Firstly, note that $\\text{Var} (y ; \\eta) = \\mathbb{E}(y^2 ; \\eta) - (\\mathbb{E}(y ; \\eta))^2$, and we already computed $\\mathbb{E}(y ; \\eta) = \\partial a / \\partial \\eta$ in the previous part. So we are left with computing $\\mathbb{E}(y^2 ; \\eta)$. Specifically, want to show $\\mathbb{E}(y^2 ; \\eta) = \\partial^2 a / \\partial \\eta^2 + (\\partial a / \\partial \\eta)^2$. To do this we utilise the exact same strategy as before.\n",
       "\n",
       "\\begin{align*}\n",
       "\\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty} y p(y; \\eta) dy &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} y p(y; \\eta) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} y b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} y b(y) \\exp(\\eta y - a(\\eta)) (y - \\frac{\\partial a}{\\partial \\eta}) dy \\\\\n",
       "    &= \\int_{-\\infty}^{\\infty} y^2 b(y) \\exp(\\eta y - a(\\eta)) dy - \\frac{\\partial a}{\\partial \\eta}\\int_{-\\infty}^{\\infty} y b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
       "    &= \\mathbb{E}(y^2 ; \\eta) - \\frac{\\partial a}{\\partial \\eta}\\mathbb{E}(y ; \\eta) \\\\\n",
       "    &= \\mathbb{E}(y^2 ; \\eta) - \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \\\\\n",
       "\\Rightarrow \\mathbb{E}(y^2 ; \\eta) &= \\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy + \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \\\\\n",
       "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} + \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \n",
       "\\end{align*}\n",
       "\n",
       "Then substituting into $\\text{Var} (y ; \\eta) = \\mathbb{E}(y^2 ; \\eta) - (\\mathbb{E}(y ; \\eta))^2$ we have $\\text{Var} (y ; \\eta) = \\partial^2 a / \\partial \\eta^2$.\n",
       "\n",
       "Remark: we assume $\\eta$ is a scalar as per the question's instruction, although this result holds in the vector form too, where $\\partial^2 / \\partial \\eta^2$ is the Hessian.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "Solution to 4(b)\n",
    "\n",
    "Firstly, note that $\\text{Var} (y ; \\eta) = \\mathbb{E}(y^2 ; \\eta) - (\\mathbb{E}(y ; \\eta))^2$, and we already computed $\\mathbb{E}(y ; \\eta) = \\partial a / \\partial \\eta$ in the previous part. So we are left with computing $\\mathbb{E}(y^2 ; \\eta)$. Specifically, want to show $\\mathbb{E}(y^2 ; \\eta) = \\partial^2 a / \\partial \\eta^2 + (\\partial a / \\partial \\eta)^2$. To do this we utilise the exact same strategy as before.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty} y p(y; \\eta) dy &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} y p(y; \\eta) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\eta} y b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} y b(y) \\exp(\\eta y - a(\\eta)) (y - \\frac{\\partial a}{\\partial \\eta}) dy \\\\\n",
    "    &= \\int_{-\\infty}^{\\infty} y^2 b(y) \\exp(\\eta y - a(\\eta)) dy - \\frac{\\partial a}{\\partial \\eta}\\int_{-\\infty}^{\\infty} y b(y) \\exp(\\eta y - a(\\eta)) dy \\\\\n",
    "    &= \\mathbb{E}(y^2 ; \\eta) - \\frac{\\partial a}{\\partial \\eta}\\mathbb{E}(y ; \\eta) \\\\\n",
    "    &= \\mathbb{E}(y^2 ; \\eta) - \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \\\\\n",
    "\\Rightarrow \\mathbb{E}(y^2 ; \\eta) &= \\frac{\\partial}{\\partial \\eta} \\int_{-\\infty}^{\\infty}p(y; \\eta) dy + \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \\\\\n",
    "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} + \\left(\\frac{\\partial a}{\\partial \\eta}\\right)^2 \n",
    "\\end{align*}\n",
    "\n",
    "Then substituting into $\\text{Var} (y ; \\eta) = \\mathbb{E}(y^2 ; \\eta) - (\\mathbb{E}(y ; \\eta))^2$ we have $\\text{Var} (y ; \\eta) = \\partial^2 a / \\partial \\eta^2$.\n",
    "\n",
    "Remark: we assume $\\eta$ is a scalar as per the question's instruction, although this result holds in the vector form too, where $\\partial^2 / \\partial \\eta^2$ is the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "Solution to 4(c)\n",
       "\n",
       "Recap on terminology: the likelihood function $L(\\theta)$ is defined as $L(\\eta) = p(y ; x, \\theta) = \\prod_{i=1}^m p(y^{(i)} ; x^{(i)}, \\theta)$. The log-likelihood function is defined as $\\log L(\\theta) = \\sum_{i=1}^m \\log p(y^{(i)} ; x^{(i)},  \\theta)$. The negative log-likelihood function, called $\\ell$ in the question, is defined as $\\ell(\\theta) = - \\log L(\\theta) = - \\sum_{i=1}^m \\log p(y^{(i)} ; x^{(i)}, \\theta)$.\n",
       "\n",
       "The Hessian of $\\ell (\\theta)$ is computed as follows:\n",
       "\n",
       "\\begin{align*}\n",
       "H_{jk} &= \\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k} \\\\\n",
       "    &= -\\frac{\\partial}{\\partial \\theta_j} \\frac{\\partial}{\\partial \\theta_k}\\sum_{i=1}^m \\log b(y^{(i)}) \\exp (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\\n",
       "    &= -\\frac{\\partial}{\\partial \\theta_j} \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^m \\log b(y^{(i)}) + \\log \\exp (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\\n",
       "    &= -\\frac{\\partial}{\\partial \\theta_j} \\sum_{i=1}^m \\frac{\\partial}{\\partial \\theta_k} \\log b(y^{(i)}) + \\frac{\\partial}{\\partial \\theta_k} (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\    \n",
       "    &= -\\frac{\\partial} {\\partial \\theta_j} \\sum_{i=1}^m x^{(i)}_k y^{(i)} - \\frac{\\partial a(\\theta^Tx)}{\\partial \\eta} x^{(i)}_k \\\\ \n",
       "    &= \\sum_{i=1}^m \\frac{\\partial^2 a}{\\eta^2}x^{(i)}_j x^{(i)}_k\n",
       "\\end{align*}\n",
       "\n",
       "Now we wish to show that H is positive semi-definite. To do this, we need to show that $\\forall v \\in \\mathbb{R}^n, v^T H v \\geq 0$. \n",
       "\n",
       "\\begin{align*}\n",
       "(H v)_j &= \\sum_{k=1}^n \\sum_{i=1}^m \\frac{\\partial^2 a}{\\partial \\eta^2}x^{(i)}_j x^{(i)}_k v_k \\\\\n",
       "\\Rightarrow v^T H v &= \\sum_{j=1}^n \\sum_{k=1}^n \\sum_{i=1}^m \\frac{\\partial^2 a}{\\partial \\eta^2}x^{(i)}_j x^{(i)}_k v_j v_k \\\\\n",
       "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} \\sum_{i=1}^m \\sum_{j=1}^n \\sum_{k=1}^n (x^{(i)}_j v_j)(x^{(i)}_k v_k) \\\\\n",
       "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} \\sum_{i=1}^m (v^T x^{(i)})^2 \\\\\n",
       "    & \\geq 0\n",
       "\\end{align*}\n",
       "\n",
       "That $\\sum_{i=1}^m (v^T x^{(i)})^2$ is non-negative is obvious since it is a sum of non-negative terms, and $\\partial^2 a / \\partial \\eta^2 \\geq 0$ follows from work done in the previous part: it is the variance of the distribution $p(y ; \\eta)$, which is necessarily non-negative.\n",
       "\n",
       "As the question says: this ultimately shows that the Hessian is positive semi-definite, and thus the log-likelihood function is convex, so the optimisation problem is nice because there is a unique global minimum.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "Solution to 4(c)\n",
    "\n",
    "Recap on terminology: the likelihood function $L(\\theta)$ is defined as $L(\\eta) = p(y ; x, \\theta) = \\prod_{i=1}^m p(y^{(i)} ; x^{(i)}, \\theta)$. The log-likelihood function is defined as $\\log L(\\theta) = \\sum_{i=1}^m \\log p(y^{(i)} ; x^{(i)},  \\theta)$. The negative log-likelihood function, called $\\ell$ in the question, is defined as $\\ell(\\theta) = - \\log L(\\theta) = - \\sum_{i=1}^m \\log p(y^{(i)} ; x^{(i)}, \\theta)$.\n",
    "\n",
    "The Hessian of $\\ell (\\theta)$ is computed as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "H_{jk} &= \\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k} \\\\\n",
    "    &= -\\frac{\\partial}{\\partial \\theta_j} \\frac{\\partial}{\\partial \\theta_k}\\sum_{i=1}^m \\log b(y^{(i)}) \\exp (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\\n",
    "    &= -\\frac{\\partial}{\\partial \\theta_j} \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^m \\log b(y^{(i)}) + \\log \\exp (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\\n",
    "    &= -\\frac{\\partial}{\\partial \\theta_j} \\sum_{i=1}^m \\frac{\\partial}{\\partial \\theta_k} \\log b(y^{(i)}) + \\frac{\\partial}{\\partial \\theta_k} (\\theta^T x^{(i)} y^{(i)} - a(\\theta^Tx^{(i)})) \\\\    \n",
    "    &= -\\frac{\\partial} {\\partial \\theta_j} \\sum_{i=1}^m x^{(i)}_k y^{(i)} - \\frac{\\partial a(\\theta^Tx)}{\\partial \\eta} x^{(i)}_k \\\\ \n",
    "    &= \\sum_{i=1}^m \\frac{\\partial^2 a}{\\eta^2}x^{(i)}_j x^{(i)}_k\n",
    "\\end{align*}\n",
    "\n",
    "Now we wish to show that H is positive semi-definite. To do this, we need to show that $\\forall v \\in \\mathbb{R}^n, v^T H v \\geq 0$. \n",
    "\n",
    "\\begin{align*}\n",
    "(H v)_j &= \\sum_{k=1}^n \\sum_{i=1}^m \\frac{\\partial^2 a}{\\partial \\eta^2}x^{(i)}_j x^{(i)}_k v_k \\\\\n",
    "\\Rightarrow v^T H v &= \\sum_{j=1}^n \\sum_{k=1}^n \\sum_{i=1}^m \\frac{\\partial^2 a}{\\partial \\eta^2}x^{(i)}_j x^{(i)}_k v_j v_k \\\\\n",
    "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} \\sum_{i=1}^m \\sum_{j=1}^n \\sum_{k=1}^n (x^{(i)}_j v_j)(x^{(i)}_k v_k) \\\\\n",
    "    &= \\frac{\\partial^2 a}{\\partial \\eta^2} \\sum_{i=1}^m (v^T x^{(i)})^2 \\\\\n",
    "    & \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "That $\\sum_{i=1}^m (v^T x^{(i)})^2$ is non-negative is obvious since it is a sum of non-negative terms, and $\\partial^2 a / \\partial \\eta^2 \\geq 0$ follows from work done in the previous part: it is the variance of the distribution $p(y ; \\eta)$, which is necessarily non-negative.\n",
    "\n",
    "As the question says: this ultimately shows that the Hessian is positive semi-definite, and thus the log-likelihood function is convex, so the optimisation problem is nice because there is a unique global minimum.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
