{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Solution to 2(a) </font>\n",
    "\n",
    "The definition of model being well calibrated: $$\\frac{\\sum_{i \\in I_{a, b}}h_{\\theta}(x^{(i)})}{|\\{i \\in I_{a, b}\\}|} = \\frac{\\sum_{i \\in I_{a, b}}1\\{y^{(i)}=1\\}}{|\\{i \\in I_{a,b}\\}|}$$\n",
    "\n",
    "Here $I_{a,b}$ is the set of examples $(x^{(i)}, y^{(i)})$ where $h_{\\theta}(x^{(i)}) \\in (a, b)$. The question asks to check this property holds when $h_\\theta$ is the hypothesis function found by logistic regression, for $(a, b) = (0, 1)$.\n",
    "\n",
    "The loss function for logistic regression: $\\ell (\\theta) = - \\frac{1}{m} \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))$.\n",
    "\n",
    "Differentiating with respect to $\\theta_j$ and setting to zero gives: $\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} = 0$.\n",
    "\n",
    "From the $j=0$ component, we get that $\\sum_{i=1}^m h_\\theta(x^{(i)}) = \\sum_{i=1}^m y^{(i)}$ (we know this because $x_0^{(i)} \\neq 0$ guaranteed, as it's the bias term and set to $1$). This is the result asked for.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Solution to 2(b) </font>\n",
    "\n",
    "Perfectly calibrated does not imply perfect accuracy. Consider the data $\\{(x^{(1)}, 1), (x^{(2)}, 1), (x^{(3)}, 0)\\}$ and a binary classifier $h(x^{(i)}) = 2/3$ for all $i$. This classifier is perfectly calibrated, because $2/3$ of the examples are positive, but incorrectly classifies the third example.\n",
    "\n",
    "Conversely, consider the data $\\{(x^{(1)}, 1), (x^{(2)}, 1), (x^{(3)}, 1)\\}$ and the binary classifier $h(x^{(i)}) = 2/3$ for all $i$. This classifier is not perfectly calibrated, because it is not true that $2/3$ of the examples are positive (all of them are), but it is perfectly accurate, because it correctly classifies all examples as positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Solution to 2(c) </font>\n",
    "\n",
    "The loss function for logistic regression with regularisation is different to the one given in 2(a), so the $\\theta$ found would be different to the one which produces a well-calibrated classifier, hence regularisation would not produce a well-calibrated classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15312389fa51bcb29fe9963781c4ce50a0f51189ba0dc67d71b16946675dd986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
